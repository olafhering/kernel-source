From e1fc9911e85251004306fd462b48a6063ca0c3aa Mon Sep 17 00:00:00 2001
From: Petr Pavlu <petr.pavlu@suse.com>
Date: Thu, 17 Jun 2021 17:58:52 +0200
Subject: [PATCH] PCI: hv: Use expected affinity when unmasking IRQ
Patch-mainline: Never, fix for use of effective affinity
References: bsc#1185973, bsc#1195536

Function hv_irq_unmask() in the Hyper-V PCIe MSI driver relies on
a SLE-specific implementation of irq_data_get_effective_affinity_mask()
(patches.suse/irq_data_get_effective_affinity_mask.patch) and uses its
result to unmask a given IRQ on specific CPUs. A problem is that
irq_data_get_effective_affinity_mask() can incorrectly return an
effective mask that is not a subset of the originally requested
affinity. Specifically, if a machine uses the flat APIC driver then the
effective affinity returned by irq_data_get_effective_affinity_mask() is
always 0xff. The result is that hv_irq_unmask() can unmask the IRQ on
a wrong CPU.

Fixing irq_data_get_effective_affinity_mask() would require bringing in
more backports to implement the functionality properly. However, since
the function is used only by the Hyper-V PCIe MSI driver, a minimum fix
is to correct the problem there by additionally doing intersection with
the original affinity. This must be done with some care still as an IRQ
can be temporarily assigned during its initialization to a CPU that is
not in its affinity, in which case the code must avoid doing the
intersection to make sure the IRQ gets unmasked on any CPU at all.

Signed-off-by: Petr Pavlu <petr.pavlu@suse.com>
---
 drivers/pci/host/pci-hyperv.c | 12 +++++++++++-
 1 file changed, 11 insertions(+), 1 deletion(-)

diff --git a/drivers/pci/host/pci-hyperv.c b/drivers/pci/host/pci-hyperv.c
index 1f49b50691ed..d49cd74e2abf 100644
--- a/drivers/pci/host/pci-hyperv.c
+++ b/drivers/pci/host/pci-hyperv.c
@@ -944,6 +944,7 @@ static void hv_irq_unmask(struct irq_data *data)
 	struct retarget_msi_interrupt *params;
 	struct hv_pcibus_device *hbus;
 	struct cpumask *dest;
+	cpumask_var_t tmp;
 	struct pci_bus *pbus;
 	struct pci_dev *pdev;
 	unsigned long flags;
@@ -991,13 +992,21 @@ static void hv_irq_unmask(struct irq_data *data)
 		params->int_target.flags |=
 			HV_DEVICE_INTERRUPT_TARGET_PROCESSOR_SET;
 
-		for_each_cpu_and(cpu, dest, cpu_online_mask) {
+		if (!alloc_cpumask_var(&tmp, GFP_ATOMIC)) {
+			res = 1;
+			goto exit_unlock;
+		}
+		cpumask_and(tmp, dest, irq_data_get_affinity_mask(data));
+		if (cpumask_empty(tmp))
+			cpumask_copy(tmp, dest);
+		for_each_cpu_and(cpu, tmp, cpu_online_mask) {
 			cpu_vmbus = hv_cpu_number_to_vp_number(cpu);
 
 			if (cpu_vmbus >= HV_VP_SET_BANK_COUNT_MAX * 64) {
 				dev_err(&hbus->hdev->device,
 					"too high CPU %d", cpu_vmbus);
 				res = 1;
+				free_cpumask_var(tmp);
 				goto exit_unlock;
 			}
 
@@ -1007,6 +1016,7 @@ static void hv_irq_unmask(struct irq_data *data)
 			params->int_target.vp_set.banks[index] |=
 				(1ULL << (cpu_vmbus & 63));
 		}
+		free_cpumask_var(tmp);
 		/*
 		 * var-sized hypercall, var-size starts after vp_mask (thus
 		 * vp_set.format does not count, but vp_set.valid_banks does).
-- 
2.26.2

